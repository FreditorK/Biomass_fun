{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366cc677",
   "metadata": {},
   "source": [
    "# Ecosystem Services Valuation Workflow\n",
    "\n",
    "This notebook follows the 11-phase roadmap from the project README. Fill in each section sequentially to maintain provenance and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc418d",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "- Activate the `biomass` environment (`conda activate biomass`).\n",
    "- Update the project path, AOI coordinates, and CDSE credentials in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415cf286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from rasterio.mask import mask\n",
    "from rasterio.io import MemoryFile\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "print(\"Core libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13720d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/fred/Projects/Feishu-Hack/Biomass_fun/data\n",
      "Found 2 SAFE folders inside data/:\n",
      " • S2C_MSIL2A_20251113T024511_N0511_R089_T50RQV_20251113T054013.SAFE\n",
      " • S2C_MSIL2A_20251113T024511_N0511_R089_T51RTQ_20251113T054013.SAFE\n",
      "Ancillary data directory: /home/fred/Projects/Feishu-Hack/Biomass_fun/data/ancillary\n",
      "AOI file detected: /home/fred/Projects/Feishu-Hack/Biomass_fun/data/aoi.geojson\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "SAFE_PATTERN = \"S2C_MSIL2A_*.SAFE\"\n",
    "SAFE_ROOTS = sorted(DATA_DIR.glob(SAFE_PATTERN))\n",
    "if not SAFE_ROOTS:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No Sentinel-2 SAFE directories found via pattern '{SAFE_PATTERN}' in {DATA_DIR}\"\n",
    "    )\n",
    "print(f\"Found {len(SAFE_ROOTS)} SAFE folders inside data/:\")\n",
    "for safe in SAFE_ROOTS:\n",
    "    print(\" •\", safe.name)\n",
    "\n",
    "ANCILLARY_DIR = DATA_DIR / \"ancillary\"\n",
    "ANCILLARY_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Ancillary data directory: {ANCILLARY_DIR}\")\n",
    "\n",
    "AOI_PATH = DATA_DIR / \"aoi.geojson\"\n",
    "if AOI_PATH.exists():\n",
    "    print(f\"AOI file detected: {AOI_PATH}\")\n",
    "else:\n",
    "    print(f\"AOI file not found (expected at {AOI_PATH}). Drop a GeoJSON/shapefile copy there when ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5769710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector data from /home/fred/Projects/Feishu-Hack/Biomass_fun/data/aoi.geojson (1 features, CRS=EPSG:4326)\n",
      "AOI centroid: (31.1209, 119.8241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32105/2682557146.py:12: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  AOI_CENTROID = AOI_GDF.geometry.centroid.unary_union\n",
      "/tmp/ipykernel_32105/2682557146.py:12: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  AOI_CENTROID = AOI_GDF.geometry.centroid.unary_union\n"
     ]
    }
   ],
   "source": [
    "def load_optional_vector(path: Path) -> Optional[gpd.GeoDataFrame]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    gdf = gpd.read_file(path)\n",
    "    print(f\"Loaded vector data from {path} ({len(gdf)} features, CRS={gdf.crs})\")\n",
    "    return gdf.to_crs(\"EPSG:4326\") if gdf.crs else gdf\n",
    "\n",
    "AOI_GDF = load_optional_vector(AOI_PATH)\n",
    "if AOI_GDF is None:\n",
    "    print(\"AOI_GDF not loaded yet. Provide a GeoJSON/GeoPackage/Shapefile in data/.\")\n",
    "else:\n",
    "    AOI_CENTROID = AOI_GDF.geometry.centroid.unary_union\n",
    "    print(f\"AOI centroid: ({AOI_CENTROID.y:.4f}, {AOI_CENTROID.x:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530491df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[missing] /home/fred/Projects/Feishu-Hack/Biomass_fun/data/ancillary/dem.tif\n",
      "[missing] /home/fred/Projects/Feishu-Hack/Biomass_fun/data/ancillary/slope.tif\n",
      "[missing] /home/fred/Projects/Feishu-Hack/Biomass_fun/data/ancillary/precipitation_mm_per_year.tif\n"
     ]
    }
   ],
   "source": [
    "ANCILLARY_RASTERS = {\n",
    "    \"dem\": ANCILLARY_DIR / \"dem.tif\",\n",
    "    \"slope\": ANCILLARY_DIR / \"slope.tif\",\n",
    "    \"precip\": ANCILLARY_DIR / \"precipitation_mm_per_year.tif\",\n",
    "}\n",
    "\n",
    "\n",
    "def inspect_raster(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {path}\")\n",
    "        return None\n",
    "    with rasterio.open(path) as ds:\n",
    "        print(\n",
    "            f\"[loaded] {path.name}: {ds.width}x{ds.height} px, res={ds.res}, crs={ds.crs}, nodata={ds.nodata}\"\n",
    "        )\n",
    "        return ds.profile\n",
    "\n",
    "ANCILLARY_PROFILES = {name: inspect_raster(path) for name, path in ANCILLARY_RASTERS.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc6263",
   "metadata": {},
   "source": [
    "## Phase 0: CDSE API & Download\n",
    "- Implement a reusable CDSE authentication helper (env vars or config file).\n",
    "- Load AOI centroid coordinates and build a 25 km buffer polygon (`geopandas`/`shapely`).\n",
    "- Use the CDSE search endpoint to query Sentinel-2 L2A products intersecting the AOI.\n",
    "- Filter candidates by acquisition date, cloud cover, and processing baseline.\n",
    "- Log the chosen product(s) with metadata (UUID, title, footprint, cloud %, download URL).\n",
    "- Download the `.SAFE` archives into `DATA_DIR` and verify checksums if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_name</th>\n",
       "      <th>acquisition_utc</th>\n",
       "      <th>processing_baseline</th>\n",
       "      <th>relative_orbit</th>\n",
       "      <th>tile</th>\n",
       "      <th>local_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S2C_MSIL2A_20251113T024511_N0511_R089_T50RQV_2...</td>\n",
       "      <td>2025-11-13 02:45:11</td>\n",
       "      <td>N0511</td>\n",
       "      <td>R089</td>\n",
       "      <td>T50RQV</td>\n",
       "      <td>/home/fred/Projects/Feishu-Hack/Biomass_fun/S2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2C_MSIL2A_20251113T024511_N0511_R089_T51RTQ_2...</td>\n",
       "      <td>2025-11-13 02:45:11</td>\n",
       "      <td>N0511</td>\n",
       "      <td>R089</td>\n",
       "      <td>T51RTQ</td>\n",
       "      <td>/home/fred/Projects/Feishu-Hack/Biomass_fun/S2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           safe_name     acquisition_utc  \\\n",
       "0  S2C_MSIL2A_20251113T024511_N0511_R089_T50RQV_2... 2025-11-13 02:45:11   \n",
       "1  S2C_MSIL2A_20251113T024511_N0511_R089_T51RTQ_2... 2025-11-13 02:45:11   \n",
       "\n",
       "  processing_baseline relative_orbit    tile  \\\n",
       "0               N0511           R089  T50RQV   \n",
       "1               N0511           R089  T51RTQ   \n",
       "\n",
       "                                          local_path  \n",
       "0  /home/fred/Projects/Feishu-Hack/Biomass_fun/S2...  \n",
       "1  /home/fred/Projects/Feishu-Hack/Biomass_fun/S2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    SAFE_ROOTS\n",
    "except NameError as exc:\n",
    "    raise RuntimeError(\"SAFE_ROOTS not initialized. Run the setup cell above first.\") from exc\n",
    "\n",
    "\n",
    "def summarize_safe(path: Path) -> dict:\n",
    "    tokens = path.name.split(\"_\")\n",
    "    return {\n",
    "        \"safe_name\": path.name,\n",
    "        \"acquisition_utc\": pd.to_datetime(tokens[2], format=\"%Y%m%dT%H%M%S\"),\n",
    "        \"processing_baseline\": tokens[3],\n",
    "        \"relative_orbit\": tokens[4],\n",
    "        \"tile\": tokens[5],\n",
    "        \"local_path\": path.resolve().as_posix(),\n",
    "    }\n",
    "\n",
    "product_log = pd.DataFrame([summarize_safe(p) for p in SAFE_ROOTS])\n",
    "product_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b79c1",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading & Preprocessing\n",
    "- Parse the `.SAFE` structure to locate band JP2 paths (10 m & 20 m resolutions).\n",
    "- Load B2, B3, B4, B8, B11, B12 arrays; resample 20 m bands to the 10 m reference grid.\n",
    "- Apply the scaling factor (÷10,000) and cast to `float32` reflectance.\n",
    "- Generate nodata masks (<=0) and cloud masks using `MSK_CLDPRB_20m` or SCL rasters.\n",
    "- Combine masks into a single `valid` mask to keep clean pixels for downstream phases.\n",
    "- Build quick-look products: RGB, SWIR composites, cloud overlays for QA/QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from contextlib import ExitStack\n",
    "from rasterio.merge import merge\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "BAND_SPECS = {\n",
    "    \"B02\": {\"label\": \"blue\", \"folder\": \"R10m\", \"suffix\": \"10m\"},\n",
    "    \"B03\": {\"label\": \"green\", \"folder\": \"R10m\", \"suffix\": \"10m\"},\n",
    "    \"B04\": {\"label\": \"red\", \"folder\": \"R10m\", \"suffix\": \"10m\"},\n",
    "    \"B08\": {\"label\": \"nir\", \"folder\": \"R10m\", \"suffix\": \"10m\"},\n",
    "    \"B11\": {\"label\": \"swir1\", \"folder\": \"R20m\", \"suffix\": \"20m\"},\n",
    "    \"B12\": {\"label\": \"swir2\", \"folder\": \"R20m\", \"suffix\": \"20m\"},\n",
    "}\n",
    "\n",
    "SCALING_FACTOR = 10000.0\n",
    "PIXEL_AREA_HA_10M = 0.01\n",
    "\n",
    "\n",
    "def get_granule_dir(safe_dir: Path) -> Path:\n",
    "    granules = sorted((safe_dir / \"GRANULE\").iterdir())\n",
    "    if not granules:\n",
    "        raise FileNotFoundError(f\"No GRANULE directory found in {safe_dir}\")\n",
    "    return granules[0]\n",
    "\n",
    "\n",
    "def find_band_path(safe_dir: Path, band_id: str) -> Path:\n",
    "    spec = BAND_SPECS[band_id]\n",
    "    granule = get_granule_dir(safe_dir)\n",
    "    search_root = granule / \"IMG_DATA\" / spec[\"folder\"]\n",
    "    pattern = f\"*_{band_id}_{spec['suffix']}.jp2\"\n",
    "    matches = sorted(search_root.glob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"Band {band_id} not found in {safe_dir}\")\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "def find_quality_mask(safe_dir: Path, filename: str) -> Path:\n",
    "    granule = get_granule_dir(safe_dir)\n",
    "    candidate = granule / \"QI_DATA\" / filename\n",
    "    if not candidate.exists():\n",
    "        raise FileNotFoundError(f\"Quality mask {filename} missing in {safe_dir}\")\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def merge_datasets(paths, dst_profile=None, resampling=Resampling.bilinear):\n",
    "    with ExitStack() as stack:\n",
    "        datasets = [stack.enter_context(rasterio.open(p)) for p in paths]\n",
    "        merge_kwargs = {}\n",
    "        if dst_profile is not None:\n",
    "            merge_kwargs.update(\n",
    "                dst_transform=dst_profile[\"transform\"],\n",
    "                dst_crs=dst_profile[\"crs\"],\n",
    "                dst_width=dst_profile[\"width\"],\n",
    "                dst_height=dst_profile[\"height\"],\n",
    "            )\n",
    "        mosaic, out_transform = merge(datasets, resampling=resampling, **merge_kwargs)\n",
    "        base_profile = datasets[0].profile.copy()\n",
    "    base_profile.update(\n",
    "        height=mosaic.shape[1],\n",
    "        width=mosaic.shape[2],\n",
    "        transform=out_transform,\n",
    "        crs=dst_profile[\"crs\"] if dst_profile else base_profile[\"crs\"],\n",
    "        count=1,\n",
    "        nodata=0,\n",
    "        dtype=mosaic.dtype,\n",
    "    )\n",
    "    return mosaic[0], base_profile\n",
    "\n",
    "\n",
    "def load_band(band_id: str, dst_profile=None, resampling=Resampling.bilinear):\n",
    "    paths = [find_band_path(safe, band_id) for safe in SAFE_ROOTS]\n",
    "    return merge_datasets(paths, dst_profile=dst_profile, resampling=resampling)\n",
    "\n",
    "\n",
    "def to_reflectance(array: np.ndarray) -> np.ndarray:\n",
    "    return (array.astype(np.float32) / SCALING_FACTOR).clip(0, 1.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_red, reference_profile = load_band(\"B04\")\n",
    "REFERENCE_PROFILE = {\n",
    "    \"transform\": reference_profile[\"transform\"],\n",
    "    \"crs\": reference_profile[\"crs\"],\n",
    "    \"width\": reference_profile[\"width\"],\n",
    "    \"height\": reference_profile[\"height\"],\n",
    "}\n",
    "\n",
    "BAND_DATA: Dict[str, np.ndarray] = {\"red\": to_reflectance(reference_red)}\n",
    "for band_id, spec in BAND_SPECS.items():\n",
    "    if spec[\"label\"] == \"red\":\n",
    "        continue\n",
    "    arr, _ = load_band(band_id, dst_profile=reference_profile)\n",
    "    BAND_DATA[spec[\"label\"]] = to_reflectance(arr)\n",
    "\n",
    "cloud_probability_paths = [find_quality_mask(safe, \"MSK_CLDPRB_20m.jp2\") for safe in SAFE_ROOTS]\n",
    "cloud_prob_raw, _ = merge_datasets(\n",
    "    cloud_probability_paths,\n",
    "    dst_profile=reference_profile,\n",
    "    resampling=Resampling.bilinear,\n",
    ")\n",
    "cloud_probability = cloud_prob_raw.astype(np.float32)\n",
    "\n",
    "scl_paths = [find_quality_mask(safe, \"MSK_CLASSI_B00.jp2\") for safe in SAFE_ROOTS]\n",
    "scl_raw, _ = merge_datasets(\n",
    "    scl_paths,\n",
    "    dst_profile=reference_profile,\n",
    "    resampling=Resampling.nearest,\n",
    ")\n",
    "SCL = scl_raw.astype(np.uint8)\n",
    "\n",
    "stack_for_mask = np.stack(list(BAND_DATA.values()), axis=0)\n",
    "nodata_mask = np.any(stack_for_mask <= 0.0, axis=0)\n",
    "cloud_mask = cloud_probability >= 40  # % probability threshold\n",
    "scl_cloud = np.isin(SCL, [3, 8, 9, 10, 11])\n",
    "valid_mask = ~(nodata_mask | cloud_mask | scl_cloud)\n",
    "\n",
    "MASKS = {\n",
    "    \"nodata\": nodata_mask,\n",
    "    \"cloud_prob\": cloud_mask,\n",
    "    \"scl_cloud\": scl_cloud,\n",
    "    \"valid\": valid_mask,\n",
    "    \"cloud_probability\": cloud_probability,\n",
    "    \"scl\": SCL,\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Loaded bands:\", \", \".join(sorted(BAND_DATA.keys())),\n",
    "    \"| valid pixels:\", int(valid_mask.sum()),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_align_raster(path: Path, reference_profile: dict | None = None):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)\n",
    "        profile = src.profile\n",
    "        if reference_profile is None:\n",
    "            return data, profile\n",
    "        reprojected, new_profile = merge_datasets([path], dst_profile=reference_profile, resampling=Resampling.bilinear)\n",
    "        return reprojected, new_profile\n",
    "\n",
    "\n",
    "def clip_raster_to_aoi(array: np.ndarray, profile: dict, aoi_gdf: gpd.GeoDataFrame):\n",
    "    if aoi_gdf is None or aoi_gdf.empty:\n",
    "        return array, profile\n",
    "    geoms = [geom.__geo_interface__ for geom in aoi_gdf.to_crs(profile[\"crs\"]).geometry]\n",
    "    with rasterio.Env():\n",
    "        with MemoryFile() as memfile:\n",
    "            with memfile.open(**profile) as dataset:\n",
    "                dataset.write(array, 1)\n",
    "                clipped, clipped_transform = mask(dataset, geoms, crop=True)\n",
    "    new_profile = profile.copy()\n",
    "    new_profile.update(\n",
    "        height=clipped.shape[1],\n",
    "        width=clipped.shape[2],\n",
    "        transform=clipped_transform,\n",
    "    )\n",
    "    return clipped[0], new_profile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67174d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_arrays = {}\n",
    "for name, path in ANCILLARY_RASTERS.items():\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    arr, profile = load_and_align_raster(path, reference_profile=reference_profile)\n",
    "    if AOI_GDF is not None:\n",
    "        arr, profile = clip_raster_to_aoi(arr, profile, AOI_GDF)\n",
    "    ancillary_arrays[name] = {\"array\": arr, \"profile\": profile}\n",
    "    print(f\"Aligned ancillary raster '{name}' -> shape {arr.shape}\")\n",
    "\n",
    "if not ancillary_arrays:\n",
    "    print(\"No ancillary rasters have been aligned yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3ce20",
   "metadata": {},
   "source": [
    "## Phase 2: Vegetation Indices\n",
    "- Implement NDVI, EVI, and optional SAVI with epsilon protection on denominators.\n",
    "- Clip outputs to the [-1, 1] range and set invalid pixels to `NaN` using `MASKS`.\n",
    "- Summarize each index with descriptive stats (min, max, mean, median, std, percentiles).\n",
    "- Plot map panels and histograms to confirm distributions and detect anomalies.\n",
    "- Flag out-of-range values before feeding indices into classification or biomass models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe632d",
   "metadata": {},
   "source": [
    "## Phase 3: Land Cover Classification\n",
    "- Define NDVI-based thresholds for `non-forest`, `sparse`, `moderate`, and `dense` classes.\n",
    "- Create a categorical raster using the thresholds and `MASKS[\"valid\"]`.\n",
    "- Calculate area (ha) per class using pixel area conversions.\n",
    "- Plot a classification map with an intuitive color palette.\n",
    "- Summarize counts/areas in a table for reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f6e4c",
   "metadata": {},
   "source": [
    "## Phase 4: Biomass Estimation\n",
    "- Select or cite an allometric equation suitable for the study region.\n",
    "- Apply the equation pixel-wise to generate a biomass raster (tons/ha).\n",
    "- Mask low-vegetation pixels (e.g., NDVI < 0.2) before aggregating.\n",
    "- Enforce realistic min/max biomass bounds to avoid outliers.\n",
    "- Compute total biomass (tons) using pixel area (0.01 ha at 10 m).\n",
    "- Visualize biomass distribution with a legend and summary statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5228a6e",
   "metadata": {},
   "source": [
    "## Phase 5: Analysis & Validation\n",
    "- Derive descriptive stats (mean, median, std, min, max) for biomass and key indices.\n",
    "- Build histograms/boxplots to inspect distributions and potential skew.\n",
    "- Compare aggregated values to literature benchmarks for similar ecosystems.\n",
    "- Conduct sensitivity checks on main parameters (e.g., NDVI thresholds, coefficients).\n",
    "- Document assumptions, uncertainties, and data quality caveats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d04793",
   "metadata": {},
   "source": [
    "## Phase 6: Water Detection\n",
    "- Compute NDWI and MNDWI using the reflectance bands already loaded.\n",
    "- Apply tuned thresholds to delineate water bodies.\n",
    "- Separate water classes by permanence/intensity if possible.\n",
    "- Calculate total water area and percentage of valid pixels.\n",
    "- Map the water mask(s) with transparent overlays on RGB for QA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccaa60",
   "metadata": {},
   "source": [
    "## Phase 7: Water Quality\n",
    "- Calculate a turbidity proxy such as the Normalized Difference Turbidity Index (NDTI).\n",
    "- Restrict calculations to water pixels only.\n",
    "- Create quality classes (e.g., healthy, moderate, degraded).\n",
    "- Plot maps showing turbidity gradients.\n",
    "- Note potential confounders (suspended sediment, sensor noise).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f3df4",
   "metadata": {},
   "source": [
    "## Phase 8: Hydrological Analysis\n",
    "- Build riparian buffer zones (30 m, 100 m, 300 m) using AOI geometry.\n",
    "- Summarize NDVI or biomass statistics within each buffer to assess vegetation quality.\n",
    "- Evaluate wetland connectivity metrics (distance to water, corridor quality, area).\n",
    "- Flag priority conservation corridors based on connectivity + condition.\n",
    "- Document formulas/assumptions for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238ffbe",
   "metadata": {},
   "source": [
    "## Phase 9: Ecosystem Service Quantification\n",
    "- Translate biophysical metrics to per-pixel service scores for the five services.\n",
    "- **Water Flow Regulation:** estimate water storage capacity using NDVI-based vegetation factors.\n",
    "- **Water Purification:** compute pollutant removal capacity using NDVI and wetland area fractions.\n",
    "- **Sediment Control:** approximate sediment retention via simplified USLE proxies (NDVI, buffers).\n",
    "- **Aquifer Recharge:** estimate recharge potential with precipitation + NDVI-derived infiltration.\n",
    "- **Flood Protection:** approximate flood storage using floodplain extent, storage depth, roughness.\n",
    "- Produce maps and tables for each service, highlight hotspots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17cd6e",
   "metadata": {},
   "source": [
    "## Phase 10: Dynamic Ecosystem Service Valuation\n",
    "- Assign base value coefficients ($/ha/year) to each service (per README guidance).\n",
    "- Derive Quality, Scarcity, and Benefit multipliers from indices and contextual data.\n",
    "- Compute per-pixel dynamic value = base × quality × scarcity × benefit.\n",
    "- Aggregate to total annual value and unit value (per ha) for the AOI.\n",
    "- Generate valuation maps and concise summary tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f550f1e",
   "metadata": {},
   "source": [
    "## Phase 11: Documentation & Presentation\n",
    "- Write a clear methodology narrative covering input data, preprocessing, models, and assumptions.\n",
    "- Export publication-ready figures (maps, histograms, tables) with consistent styling.\n",
    "- Compile a concise PDF report (3–5 pages) summarizing objectives, methods, results, discussion.\n",
    "- Prepare a short slide deck (≈5 minutes) highlighting key insights and visuals.\n",
    "- Ensure notebook cells are clean, commented, and reproducible for final submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
